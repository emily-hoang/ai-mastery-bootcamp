from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalise the data
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# One-hote encode labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print(f"Training Data Shape: {X_train.shape}")
print(f"Testing Data Shape: {X_test.shape}")

# Define the Convolutional Neural Network (CNN) model using TensorFlow/Keras — often used for image classification (e.g., MNIST handwritten digits).
# This CNN performs: Feature extraction, Conv2D + MaxPooling2D, Flattening, Flatten, Classification, Dense → Dropout → Dense(softmax)
# This is a classic architecture for small image classification tasks and a very common way to train models on MNIST.    
model = Sequential([
  Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
  MaxPooling2D(2, 2),
  Flatten(),
  Dense(128, activation='relu'),
  Dropout(0.5),
  Dense(10, activation='softmax')
])

# Display model architecture
model.summary()

# Compile the model
model.compile(
  optimizer="adam",
  loss="categorical_crossentropy",
  metrics=['accuracy']
)

# Train the model
history = model.fit(
  X_train, y_train,
  epochs=10,
  batch_size=32,
  validation_split=0.2
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy}:.4f")

# Save the trained model
model.save('mnist_classifier.keras')

# Load the trained model
from tensorflow.keras.models import load_model
loaded_model = load_model('mnist_classifier.h5')

# Verify loaded model performance
loss, accuracy = loaded_model.evaluate(X_test, y_test)
print(f"Loaded Mode Test Accuracy: {accuracy}")

